{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapters 1 & 2\n",
    "\n",
    "## 1 - Introduction\n",
    "- Statistical learning is a set of tools that helps us understand data.\n",
    "    - Supervised: Predict or classify an outputs using inputs.\n",
    "        - eg. Using years of study to predict wages.\n",
    "    - Unsupervised: Learn more about the data, using without having outputs.\n",
    "        - eg. Discovering clusters within the data.\n",
    "\n",
    "- Statistical learning is important because with more and more data, it's uses are now beyond the academia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Statistical Learning\n",
    "\n",
    "### What is Statistical Learning\n",
    "Suppose we have our variable of interest $y$ and a set of features $X$. Statistical learning are tools that helps us to estimate the populational (and sometimes purely theorical) function $y = f(x) + \\varepsilon$.\n",
    "\n",
    "As it's not possible to discover the **real** function $f$, we estimate the function with $\\hat{y} = \\hat{f}(x)$.\n",
    "\n",
    "#### Why estimate $f$?\n",
    "\n",
    "##### Prediction\n",
    "When we have our features set and want to predict the future values of the response variable. In this case a tradeoff will likely always occur: model interpretability and prediction power.\n",
    "\n",
    "When our estimated $\\hat{f}$ is complex and it's not feasible to understand, we have what it's called a *black box*. In this case the only objective is to maximize the prediction power.\n",
    "\n",
    "- When dealing with prediction, even with a theoretical perfect estimate of $f = \\hat{f}$, there will always be some error called _irreductible error_. This error comes from the population function $f$, and per definition, is not a function o the features set $X$.\n",
    "\n",
    "##### Inference \n",
    "Sometimes we want to deeply understand the causality between events and our variable of interest. In this case, we can't simply throw a _black box_ model in our data and hope to magicaly understand the process. So we choose simpler models to be able to interpret the results from it.\n",
    "\n",
    "e.g. If a company that sells a renewable subscription to a software wants to know what are the events that are more likely to *cause* a renewal, it can't just say that a variable is the _cause_ beecause it helps the most to predict the outcome. For example, there may be a variable called _bugs reported_ and it is highly correlated with the subscription renewal. Should the company implement more bugs willingly? Probably not, what is most likely happening is that the users that uses the most the software, and have a higher necessity for it, reports more bugs because: they enmcounter more bugs or they need the bug removed to use the software properly.\n",
    "\n",
    "In this case, we need to be careful with the model results so we don't make mistakes.\n",
    "\n",
    "#### How do we estimate $f$?\n",
    "- We need to find a function $f$ that $Y \\approx \\hat{f}(X)$ for every (X,Y).\n",
    "\n",
    "##### Parametric\n",
    "Two-step approach: 1. Assume a form for $f$; 2. Estimate it's parameters.\n",
    "\n",
    "How does it work? When defining a form of $f$, our problem is not estimating $f$ anymore, it is estimating $f$'s parameters. This way, our problem is simplified.\n",
    "\n",
    "e.g. Assume that $f$ is linear in $X$: this way, we can write that $y = \\beta_0 + \\beta_1*x_1 + ...$ and simply estimate our $\\beta$s.\n",
    "\n",
    "\n",
    "##### Non-Parametric\n",
    "In this case we do not assume a form for $f$. The models try to estimate a function $f$ that approximates as much as possible the data, without being too wiggle or too rough. The benefits are clear: without the assumption the models can adapt to be any function possible. When making assumptions, it is not possible to be 100% sure that the assumption we make is the correct form of $f$, as $f$ is not observable.\n",
    "\n",
    "The major drawback is the necessity of a higher quantity of data — way more than the necessary for a parametric approach — to have accurate estimates.\n",
    "\n",
    "#### The Trade-Off Between Prediction Accuracy and Model Interpretability\n",
    "Basically, it says that models are in between a line of interpretability and flexibility. Models that are inflexible have higher interpretability, like the linear regression. \n",
    "\n",
    "This way it is normal to think that when dealing with *prediction* it is preferable to choose the most flexible and not interpretable model as possible and in the case of *inference* the most inflexible model. \n",
    "\n",
    "However, it's not always that a flexible model outperforms a inflexible one, most of the time because \n",
    "\n",
    "#### Supervised Versus Unsupervised Learning\n",
    "When not _observing_ the output variable in our training data, we cannot train models for prediction or inference. However, it is possible to get a better understading of the relationships in the data with **Unsupervised Learning**. One possible tool is called *cluster analysis*, with the most famous model being the K-Means.\n",
    "\n",
    "##### Semi-supervised learning\n",
    "When there are observations with the target variable and without it in the same dataset. The solution here is using semi-supervised learning models so that it can incorporate the information of the training target variable values as well as the information of the observations without the target. \n",
    "\n",
    "This situation can happen when it is expensive to collect the variable of interest while it is cheap to collect the predictors.\n",
    "***\n",
    "#### Regression Versus Classification Problems\n",
    "In simple terms, regression means that our variable of interest is quantitative, while in classification problems it is qualitative. However, the difference is not alway clear: in a logistic regression (for qualitative targets), it can also be interpreted as predicting the probability of a class, that is quantitative.\n",
    "\n",
    "### Assessing Model Accuracy\n",
    "*There is no free lunch:* No model outperforms every other in every single dataset. We have to test a lot of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
